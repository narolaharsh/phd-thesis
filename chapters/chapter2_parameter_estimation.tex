A GW signal emmited during the merger of two compact objects can be typically modelled using 15 parameters. Eight of them are considered intrinsic parameters; two for the masses and six for three dimensional spins of the two objects. The rest of them, e.g. sky location, distance, are considered extrinsic parameters. If one of the compact object is a neutron star, an additonal intrinsic parameter is used to model its tidal deformibility. Measurement of these parameters is a key step which enables various physics related insigts from the data, e.g. testing the prediction of the general relativiy, constraining the neutron star equation of state. In this chapter, I illustrate how the parameters of merging binary black holes and/or neutron stars are measured from a GW signal. 

\section{Bayesian analysis}

Measuring the parameters of a GW source amounts to treating each parameter as a random variable and estimating their probability distribution which is condition on the observed data. For the sake of the argument, let us assume that we are interested in measuring the total mass (sum of mass of each compact object) of the source. Before the GW signal is observed, the measurement cannot be made or we can say that all values of total mass ($M_{\mathrm{total}}$) are equally likely or the \textit{prior} probability distribution of the random variable $M_{\mathrm{total}}$ is uniform. After the signal is observed, we can update the the prior distribution using the information from signal to obtain the \textit{posterior} probability distribution. The posterior probability distribution or the \textit{measurement} of the parameter can be thought of as its probability distribution which is conditioned on the observed data. FIXME: Add a figure to show how the prior transforms to posterior.

We formalize the problem of converting the prior probability distribution to the posterior probability distribution using the Bayes theorem. Our aim is to estimate the posterior distribution $p(\vec{\theta} | \vec{d})$ where $p$ denotes the probability distribution function, $\vec{\theta}$ denotes the set of GW source parameters, and $\vec{d}$ denotes the data. Using Bayes' theorem we can invert the conditional probability as follows
\begin{equation}
    p(\vec{\theta} | \vec{d}) = \frac{\pi(\vec{\theta}) p(\vec{d} | \vec{\theta})}{p(\vec{d})},
\end{equation}
where $\pi(\vec{\theta})$ is the prior distribution on $\vec{\theta}$ which is typically assumed to be uniform. The term $p(\vec{d} | \vec{\theta})$ is the likelihood of the data given the parameters $\vec{\theta}$. The term $p(\vec{d})$ can be treated as the normalization term for the time being. This term is typically known as the evidence if we use Bayes theorem to test competing hypothesis. 

\section{Likelihood function}
While the Bayesian framework described above is conceptually easy, it is still a computationally expensive task to perform the analysis due to the dimensionality of the parameter space of $\vec{\theta}$ (typically 15 to 17), length of the $\vec{d}$, and complexity of simulating of GW signal model. Before diving into optimizing the comutation of the likelihood and exploration of the large parameter space, let us define a few standard quantites used in computing the likelihood including the likelihood function itself,

\begin{equation}
    \ln{p(\vec{d} | \vec{\theta})} = -\frac{1}{2}\left<\vec{d} - \vec{h}(\vec{\theta}) | \vec{d} - \vec{h}(\vec{\theta})\right>.
\end{equation}
Here, $\vec{h}$ denotes the GW signal model computed for the source paraemters $\vec{\theta}$. The brackets $\left<.|.\right>$ denote the noise weighted inner product 
\begin{equation}
    \left<a|b\right> = \frac{4}{T} \sum_{f} \frac{\tilde{a}(f)\tilde{b}^{*}(f)}{S_{n}(f)}.
\end{equation}
The parameter $T$ is the duration of segment $\vec{d}$ we want to analyze, $\tilde{b}^{*}$ denotes the complex conjugate of the Fourier transform of time domain vector $b$, and $f$ is the frequency. The term $S_n(f)$ is the power spectral density of the noise.

The expression of the likelihood function is inspired from the so-called Whittle likelihood which is typically used to analyze Gaussian and stationary time series. For the short stretches of data $\vec{d}$ that we analyze, the Gaussian-stationary approximation works well which makes the Whittle likelihood a suitable choice. In the hidnsight, it can be verified that the noise component at each frequency $f$ follows Gaussian distribution with mean 0 and the standard deviation given by $S_n(f)$. FIXME Figure shows the distribution of detector data.

The majority of the cost is acquired while evaluating the likelihood function on large parameter space. For transient GW signals, each likelihood evaluation takes $\mathcal{O}{10}$ milliseconds and exploring the full parameter space typically requires $\mathcal{O}(10^9)$ evaluations. For long duration and loud signals expected to be observable by 3G detectors, the cost quickly becomes intractable.  

Nested sampling. 

\begin{itemize}
    \item Nested sampling
\end{itemize}
