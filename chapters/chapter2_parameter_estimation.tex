Gravitational wave signals emitted by the merger of two compact objects are routinely detected by the current generation of GW detectors. Such a GW signal can typically be modelled using 15 parameters. Eight of them are considered intrinsic parameters; two for the masses and six for the three-dimensional spins of the two objects. The rest of them, e.g. sky location and distance, are considered extrinsic parameters. If one of the compact objects is a neutron star, an additional intrinsic parameter is added to model its tidal deformability. Measurement of these parameters is a key step which enables various physics-related insights from the data, e.g. testing the prediction of general relativity, constraining the neutron star equation of state. In this chapter, I illustrate how the parameters of merging binary black holes and/or neutron stars are measured from a GW signal. 

\section{What counts as a measurement?}
\begin{figure}
    \centering
    \includegraphics[width=8cm]{../figures/prior_posterior.pdf}
    \caption{\asam{Might want to mention for Fig. 2.1 that 3 detectors were used (I'm guessing).} An illustration of the probability distribution of the prior (blue), likelihood (red), and posterior (green) of the luminosity distance $D_{\mathrm{L}}$. Measuring a source parameter amounts to transforming the prior distribution to the posterior distribution using the likelihood of the parameter at the given value. The prior on the luminosity distance is assumed to be uniform in comoving volume therefore its values increase with $D_{\mathrm{L}}$.}
    \label{fig:prior_post}
\end{figure}

\iffalse
\begin{wrapfigure}{l}{0.5\textwidth}
 \centering
 \includegraphics[width=8cm]{../figures/prior_posterior.pdf}
 \caption{An illustration of the probability distribution of the prior (blue), likelihood (red), and posterior (green) of the luminosity distance $D_{\mathrm{L}}$. Measuring a source parameter amounts to transforming the prior distribution to the posterior distribution using the likelihood of the parameter at the given value. The prior on the luminosity distance is assumed to be uniform in comoving volume therefore its values increase with $D_{\mathrm{L}}$.}
 \label{fig:prior_post}
\end{wrapfigure}
\fi

Measuring the parameters of a GW source amounts to treating each parameter as a random variable and estimating its probability distribution which is conditioned on the observed data. For the sake of the argument, let's say we are interested in measuring the luminosity distance $(D_{\mathrm{L}})$ to the source. Before the signal is observed, the measurement cannot be made or we can say that all values of $D_{\mathrm{L}}$ are equally likely or the \textit{prior} probability distribution of the random variable is uniform. After the data is collected i.e. a GW signal is observed, we can update the prior probability distribution using the data to obtain the \textit{posterior} probability distribution. The posterior probability distribution or the \textit{measurement} of the parameter can be thought of as the probability distribution which is conditioned on the observed data. 

\subsection{Bayesian analysis}
We formalize the problem of converting the prior probability distribution to the posterior probability distribution using the Bayes theorem. We aim to estimate the posterior distribution $p(\vec{\theta} | \vec{d})$ where $p$ denotes the probability distribution function, $\vec{\theta}$ denotes the set of GW source parameters, and $\vec{d}$ denotes the data. Using Bayes' theorem we can invert the conditional probability $p(\vec{\theta} | \vec{d})$ as,
\begin{equation}
    \label{eq:bayes}
 p(\vec{\theta} | \vec{d}) = \frac{\pi(\vec{\theta})~p(\vec{d} | \vec{\theta})}{p(\vec{d})},
\end{equation}
where $\pi(\vec{\theta})$ is the prior probability distribution of $\vec{\theta}$ which is typically assumed to be uniform. The term $p(\vec{d} | \vec{\theta})$ is the likelihood of observing data given the parameters $\vec{\theta}$. The term $p(\vec{d})$ can be treated as the normalization term for the time being. This term is typically known as the evidence if we use Bayes's theorem to test competing hypotheses. Figure~\ref{fig:prior_post} shows an illustration of the prior probability distribution (blue) and the posterior probability distribution (red) of the parameter $(D_{\mathrm{L}})$ where the latter is obtained by updating the prior weights by the weight of likelihood (green) for the corresponding value.

\subsection{Gravitational wave likelihood}
While the Bayesian framework described above is conceptually easy, it is a computationally challenging task to do parameter estimation due to the dimensionality of the space of $\vec{\theta}$ (typically 15 to 17), length of the $\vec{d}$, and complexity of simulating of GW signal models. Before diving into how we tackle these issues, let us define a few standard quantities used during the computation, starting with the likelihood function $p(\vec{d} | \vec{\theta})$ itself;

\begin{figure}
    \centering
    \includegraphics[width=8cm]{../figures/white_noise_distribution.pdf}
    \caption{To test the Gaussian-stationary nature, we compare the distribution of whitened data (blue) with the normal distribution (red). We used 16 seconds of the data segment from LIGO-Livingstone interferometer and the PSD is generated using 2048 seconds of data near the segment.}
    \label{fig:white_data}

\end{figure}

\iffalse
\begin{wrapfigure}{l}{0.5\textwidth}
 \centering
 \includegraphics[width=8cm]{../figures/white_noise_distribution.pdf}
 \caption{To test the Gaussian-stationary nature, we compare the distribution of whitened data (blue) with the normal distribution (red). We used 16 seconds of the data segment from LIGO-Livingstone interferometer and the PSD is generated using 2048 seconds of data near the segment.}
 \label{fig:white_data}
\end{wrapfigure}
\fi

\begin{equation}
    \label{eq:likelihood}
 \ln{p(\vec{d} | \vec{\theta})} = -\frac{1}{2}\left<\vec{d} - \vec{h}(\vec{\theta}) | \vec{d} - \vec{h}(\vec{\theta})\right>,
\end{equation}
where $\vec{h}$ denotes the GW signal as a function of the source parameters $\vec{\theta}$, in the detector frame. The brackets $\left<.|.\right>$ denote the noise weighted inner product; 
\begin{equation}
    \label{eq:nwe}
 <\vec{a}|\vec{b}> = \frac{4}{T}~\sum_{f}~\frac{\tilde{a}(f)~\tilde{b}^{*}(f)}{S_{n}(f)},
\end{equation}
where $T$ is the duration of segment $\vec{d}$ we want to analyze, $\tilde{b}^{*}$ denotes the complex conjugate of the Fourier transform of time domain vector $\vec{b}$, and $f$ is the frequency. The term $S_n(f)$ is the variance of the noise at frequency $f$, also known as the power spectral density (PSD). The expression for the log likelihood in Eq.~\eqref{eq:likelihood} is for one detector. For a network of detectors, it turns into a sum of log likelihood for each detector in the network. 

\subsection{Gaussian noise approximation}
The expression of the likelihood function is inspired by the so-called Whittle likelihood which is typically used to analyze Gaussian and stationary time series. While the data $\vec{d}$ cannot be expected to be Gaussian and stationary at all times, for the short stretches of data that we analyze, it can be approximated with a Gaussian distribution with zero mean and known variance, making the Whittle likelihood a suitable choice. The likelihood expression in Eq. \eqref{eq:likelihood} is simply the product (or sum when operated with natural logarithm on both sides) of Gaussian distributions with zero mean and variance equal to $S_n(f)$ for each frequency. We show a qualitative test of the Gaussian and stationary nature of the data in Figure~\ref{fig:white_data}. The distribution of the whitned data $\frac{\tilde{d}}{\sqrt{S_n}}$ tends to follow the normal distribution. The data segment used to make this plot lies near the GW150914 signal, the first direct detection of a GW signal from the merger of two black holes. 

\section{Waveforms}\begin{figure}
    \centering
    \includegraphics[width=16cm]{../figures/phenomd_xphm.pdf}
    \caption{The whitened detector frame waveform corresponding to the maximum likelihood parameters of GW190814. We show two waveforms of the phenomenological family, IMRPhenomD (blue)and IMRPhenomXPHM (red). The former only contains the dominant mode and aligned spin description. The latter presents a more complete description, i.e. contribution from dominant and higher-order modes and all three spins components. The shades of grey approximately show different regimes in which different methods are used to compute the phase and amplitude.}
    \label{fig:phenom_wav}
\end{figure}
Waveforms are the solutions to Einstein's field equations. They describe how the spacetime evolves during the coalescence of two compact objects. A solution to the field equation that describes the complete phenomenon of coalescence, i.e. inspiral, merger, and ringdown (IMR), cannot be obtained analytically. One typically resorts to numerical relativity simulations(see for a review~\cite{Cardoso:2014uka}). Ideally, we would like to have waveforms obtained from the numerical relativity simulation for the entire parameter space of $\vec{\theta}$. However, these simulations are computationally expensive, which makes them unsuitable for parameter inference, where we need to simulate waveforms for a large number of $(10^6-10^7)$ $\vec{\theta}$ per analysis. Various methods have been developed over the past decade to address this challenge, which in turn give rise to different waveform families. 

In the latter chapter of this thesis, I have used the IMRPhenom waveforms. It is a family of phenomenological waveforms that contains a complete description of the coalescence of a compact binary. The phenomenological waveforms are constructed using different ansatzes for phase and amplitude of the waveforms in different regimes (see~Figure~\ref{fig:phenom_wav}). The waveforms are phenomenological in the sense that the parameters appearing in the ansatz are phenomenological in nature~\cite{Ajith:2009bn, Santamaria:2010yb}. The phenomenological parameters are calibrated against a set of numerical relativity simulations. This approach produces accurate and computationally inexpensive waveforms, suitable for parameter inference. 

In the case of binary black holes (BBHs), we typically need 15 parameters describe the phenomena; two for the masses of the two black holes, six for the three dimensional spins for the two black holes, three for the volume localisation (right ascension, declination, and luminosity distance w.r.t to the detector), zenith angle between line of sight and total angular momentum, polarisation angle, phase at the time of coalescence, and time of coalescence. If there is a binary neutron star, we need to add a parameter to account for the tidal deformability of the object. The IMRPhenomD waveform of the phenomenological family describes the systems with aligned (or anti-aligned) spins and with the dominant mode contribution~\cite{Khan:2015jqa, Husa:2015iqa}. A more complete description is provided by IMRPhenomXPHM waveform, which describes a system with precessing spins (all three spin components) and with contribution from dominant as well as higher order modes~\cite{Pratten:2020ceb}.  


\section{Nested sampling}
When performing parameter estimation, we need to explore the likelihood function on large parameter space. For example, if we choose only 10 points along each dimension of the parameter space, we need to perform about $10^{15}$ to $10^{17}$ likelihood evaluation where each evaluation takes $\mathcal{O}(10)$ milliseconds. Indeed, 10 points may be too sparse to cover each dimension. Therefore, we rely on various sampling algorithms to efficiently explore a large parameter space. Here, I explain Nested sampling, an algorithm routinely used for parameter estimation and also the one used to obtain the majority of the results in the later chapters of this thesis. 

%\subsection{Nested sampling}
\begin{figure}
    \centering
    \includegraphics[width=16cm]{../figures/nested_sampling.pdf}
    \caption{\textit{Left}: Constant likelihood surfaces for a two dimensional parameter space $\vec{\theta} = \{\theta_1, \theta_2\}$, illustrating the nested contours. \textit{Right}: Evolution of the prior mass ($X$) enclosed by the constant likelihood surfaces shown in the left plot.}
    \label{fig:nests}
\end{figure}
The nested sampling algorithm was introduced in 2004 by John Skilling~\cite{Skilling:2006gxv} and it was first applied to analyze GW signals by Veitch and Vecchio~\cite{Veitch:2009hd}. The name derives from the fact that the progression of the algorithm relies on \textit{nested} contours of likelihood (Figure~\ref{fig:nests} (left)) and not the individual likelihood values. The algorithm aims to compute the evidence~$Z$, 
\begin{equation}
\label{eq:evidence}
 Z = \int~\mathrm{d}\vec{\theta}~\pi(\vec{\theta})~p(\vec{d} | \vec{\theta})
\end{equation}
%    %&= \int~\mathrm{d}\vec{\theta}~p(\vec{d}| \vec{\theta})~\pi(\vec{\theta})
where the integrand on the right-hand side is simply the product of the likelihood and the prior defined in Eq. \eqref{eq:bayes}, i.e., $Z$ is the normalization constant from that equation.
The posterior distribution on the model parameters, $p(\vec{\theta} | \vec{d})$, is obtained as a byproduct. In this sense, Nested sampling is an algorithm to perform multidimensional integral.  

Typically, one can divide the parameter space of $\Delta\vec{\theta}$ into smaller cubes such that the integrand nearly remains constant over the cube and then sum over the whole space to obtain $Z$. However, this approach quickly becomes intractable. Here, we instead divide the space into contours created by constant likelihood surfaces (Figure~\ref{fig:nests}). For simplicity let us denote the likelihood term $p(\vec{d} | \vec{\theta})$ by $\mathcal{L}(\vec{\theta})$. %The left plots illustrates three surfaces of constant likelihoods, $\mathcal{L}_1 < \mathcal{L}_2 < \mathcal{L}_3$. 
Let us define $X(\lambda)$ as,
\begin{equation}
    \label{eq:prior_mass}
X(\lambda) = \int_{\mathcal{L}(\vec{\theta}) > \lambda}~\pi(\vec{\theta})~\mathrm{d}\vec{\theta},
\end{equation}
which is the prior mass obtained by integrating $\pi(\vec{\theta})$ over the region of parameter space where $\mathcal{L}(\vec{\theta}) > \lambda$. Since the prior is a normalized probability distribution, its integration over the entire space spanned by $\vec{\theta}$ is 1, therefore the maximum value of $X$ is also 1. As $\lambda$ increases, $X$ tends to zero as the volume over which prior needs to be integrated grows smaller (Figure~\ref{fig:nests}) and smaller. Using Eq. \eqref{eq:prior_mass}, we can convert the multidimensional integral of Eq. \eqref{eq:evidence} into one dimensional integration, 
\begin{equation}
    \label{eq:oned}
 Z = \int_0^{1} \mathrm{d} X~\hat{\mathcal{L}}(X),
\end{equation}
where $\hat{\mathcal{L}}$ is the inverse of $X(\lambda)$,
$$
\hat{\mathcal{L}} (X (\lambda)) \equiv \lambda.
$$
We can approximate the integral in Eq.~\eqref{eq:oned} with the Riemann sum to obtain
\begin{equation}
 Z \approx \sum_{k=0}^{N}  \left(\frac{\hat{\mathcal{L}}_k + \hat{\mathcal{L}}_{k+1}}{2}\right) \Delta{X_k} \equiv \sum_{k=0}^{N} w_k.
\end{equation}
Once we calculate $Z$, the posterior probability of point $\vec{\theta}_k$ can be computed by
\begin{equation}
 p(\vec{\theta}_k | \vec{d}) = \frac{\pi(\theta) \mathcal{L}(\theta)}{Z} = \frac{w_k}{\sum_{k=0}^{N} w_k},
\end{equation}
as mentioned near the beginning of this sub-section. 

Although we have defined prior mass X, we have so far not specified how it is calculated once the sampling starts. The quantity X is non-increasing in nature and its value ranges from 1 to 0. Treating it as a random variable, its probability distribution can be approximated with the uniform distribution,  
\begin{equation}X \sim U(0, 1)\end{equation} and the corresponding cumulative distribution function is,
\begin{equation}
 F(X) = \int_{0}^{X} \mathrm{d} X' = X.
\end{equation}
The probability that a random variable $\chi$ is greater than the prior masses from the full set of prior masses $\{X_i\}$ is
\begin{equation}
P(\chi > \{X_i\}) = \Pi_{i=0}^{N} F(X_i = \chi) = \chi^K,
\end{equation}
where we assume that the samples are independent of each other. The probability density $p(\chi)$ is then given by 
\begin{equation}
 p(\chi) = \frac{d}{d\chi} P(\chi > \{X_i\}) = K \chi^{K-1}.
\end{equation}
The above equality suggests that random variable $\chi$ indeed follows the Beta distribution with
\begin{equation}
\chi \sim B(K, 1).
\end{equation}
We have assumed that $X$ and $\chi$ range from 0 and 1. However, as the algorithm progresses, the prior mass $X$ and, in turn, $\chi$ shrinks. Let's say they have an upper bounded of $X^{*}$, then we can define a shrinkage ratio $t\equiv \chi / X^{*}$ where
\begin{equation}
 t \sim B(K, 1).
\end{equation}
We provide the pseudocode of how the nested sampling progresses in Algorithm block~\ref{alg:nes_sam}. The pseudocode is generalized for K number of live points. For a typical parameter estimation run, one uses $\mathcal{O}(10^3)$ live points. 
\RestyleAlgo{ruled}
\begin{algorithm}
    \SetKwComment{Comment}{//~}{}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \caption{Nested Sampling}\label{alg:nes_sam}
    \Input{Likelihood function $\mathcal{L}(\vec{\theta})$, prior distribution $\pi(\vec{\theta})$, and number of live points $K$}
    \Output{Evidence estimate $Z$ and posterior samples $p(\vec{\theta} | \vec{d})$}

    \Comment{Initialize the algorithm}
 Choose $K$ live points $\{\vec{\theta}_1, \vec{\theta}_2, ..., \vec{\theta}_K\}$ from the prior $\pi(\vec{\theta})$, compute likelihoods $\{\mathcal{L}_k = \mathcal{L}(\vec{\theta}_k)\}$, set initial prior volume $X_0 = 1$, iteration $i = 0$, and $Z = 0$\;
    \Comment{Starting main loop}
    \While{termination criterion not met}
 {
 Identify the point $\vec{\theta}_i$ corresponding lowest likelihood, $\mathcal{L}_i = \min \{\mathcal{L}_k\}$\;
        \eIf{$i==0$}
 {Draw $X_i$ from $\mathcal{B}(K, 1)$\;} 
 {{Draw $t$ from $\mathcal{B}(K, 1)$\; $X_i = tX_{i-1}$\;}}

 Discard the $\vec{\theta}_i$ as a \textit{dead} point, collect the tuple ($\mathcal{L}_i$, $X_i$) corresponding to $\vec{\theta}_i$;
 Increase Z by adding $\Delta Z =  \frac{1}{2}\left(\mathcal{L}_i + \mathcal{L}_{i-1}\right) \left(X_i - X_{i-1}\right)$\;
 Evaluate termination criterion\;
        \eIf{criterion not met}
 {Sample a new point from the prior such that its likelihood is higher than the point that was just discarded. Add it to the set of live points so that the number of live points is equal to K again\;
        $i = i+1$\;}{{criterion met}}
        
 }
\end{algorithm}
\subsubsection{Terminating criterion}
Since the aim of the Nested sampling algorithm is to accumulate the evidence Z in small increments of $\Delta Z$, it may be terminated when the increment is smaller than a user-specified number $\epsilon$,
\begin{equation}
 \ln{\left(Z_i + X_i \mathcal{L}_{\mathrm{max}}\right)} - \ln{(Z_i)} < \epsilon,
\end{equation}
where $\mathcal{L}_{\mathrm{max}}$ is the maximum value of likelihood encountered during sampling. 

\subsection{Limitations of modelled reconstruction}
While Nested sampling has become the preferred choice of sampling algorithm for parameter estimation of GW signals~\cite{Speagle:2019ivv, Ashton:2018jfp}, it certainly still has some aspects that can be improved upon. 
\begin{enumerate}
    \item Sampling over a variable parameter space, i.e., when the dimension of the model itself is an unknown parameter, is not straightforward. 
    \item The stopping criteria of the algorithm are somewhat arbitrary. 
    \item The algorithm scales poorly as the dimensions of the model increase.
    \item Hard to achieve independent samples.
\end{enumerate}

